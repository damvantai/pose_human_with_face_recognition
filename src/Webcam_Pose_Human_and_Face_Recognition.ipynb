{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use input size 416:\n",
    "- file cfg\n",
    "- yolo3_weight.h5 when convert\n",
    "- file train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import cv2\n",
    "import dlib\n",
    "# import face_recognition\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import WebcamVideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# import cv2\n",
    "# import imutils\n",
    "# class WebcamVideoStream:\n",
    "#     def __init__(self, src=0):\n",
    "#         self.stream = cv2.VideoCapture(src)\n",
    "#         self.stream.set(3, 800)\n",
    "#         self.stream.set(4, 600)\n",
    "#         (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "#         self.stopped = False\n",
    "        \n",
    "#     def start(self):\n",
    "#         # Start the thread to read frames from the video stream\n",
    "#         Thread(target=self.update, args=()).start()\n",
    "#         return self\n",
    "    \n",
    "#     def update(self):\n",
    "#         while True:\n",
    "#             if self.stopped:\n",
    "#                 return\n",
    "            \n",
    "#             (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "#     def read(self):\n",
    "#         return self.frame\n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.stopped = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5'\n",
    "model_path = '../logs/human_pose_dataset_2388_size_416_batch_size_4/human_pose_dataset_2388_size_416_batch_size_4.h5'\n",
    "anchors_path = '../model_data/yolo_anchors.txt'\n",
    "classes_path = '../model_data/human_pose.txt'\n",
    "score = 0.4\n",
    "iou = 0.1\n",
    "model_image_size = (416, 416)\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class\n",
    "classes_path = os.path.expanduser(classes_path)\n",
    "with open(classes_path) as f:\n",
    "    class_names = f.readlines()\n",
    "\n",
    "class_names = [c.strip() for c in class_names]\n",
    "\n",
    "# Anchors\n",
    "anchors_path = os.path.expanduser(anchors_path)\n",
    "with open(anchors_path) as f:\n",
    "    anchors = f.readline()\n",
    "anchors = [float(x) for x in anchors.split(',')]\n",
    "anchors = np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logs/human_pose_dataset_2388_size_416_batch_size_4/human_pose_dataset_2388_size_416_batch_size_4.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = os.path.expanduser(model_path)\n",
    "assert model_path.endswith('.h5'), 'Keras model end with file .h5'\n",
    "\n",
    "num_anchors = len(anchors)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "is_tiny_version = num_anchors==6\n",
    "try:\n",
    "    yolo_model = load_model(model_path, compile=False)\n",
    "except:\n",
    "    if is_tiny_version:\n",
    "        yolo_model = tiny_yolo_body(Input(shape=(None, None, 3)), num_anchors//2, num_classes)\n",
    "    else:\n",
    "        yolo_model = yolo_body(Input(shape=(None, None, 3)), num_anchors//3, num_classes)\n",
    "    \n",
    "    yolo_model.load_weights(model_path)\n",
    "else:\n",
    "    yolo_model.layers[-1].output_shape[-1] == num_anchors/len(yolo_model.output) * (num_classes + 5), 'Mismatch between model and given anchor and class sizes'\n",
    "    \n",
    "print(\"{} model, anchors, and classes loaded.\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_encodings_in_room = []\n",
    "face_names_in_room = []\n",
    "known_face_encodings_array = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "# Convert nparray -> List to face_encoding\n",
    "len_of_array_known_face_names = len(known_face_names)\n",
    "known_face_encodings_array = known_face_encodings_array.reshape(len_of_array_known_face_names, 128)\n",
    "known_face_encodings = []\n",
    "for i in range(len_of_array_known_face_names):\n",
    "    known_face_encodings.append(known_face_encodings_array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_image_shape = K.placeholder(shape=(2, ))\n",
    "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, len(class_names), input_image_shape, score_threshold=score, iou_threshold=iou)\n",
    "num_frame = 0\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "# Video capture\n",
    "video_capture = WebcamVideoStream(src=0).start()\n",
    "\n",
    "while True:\n",
    "    num_frame += 1\n",
    "\n",
    "    # Read video frame and flip camera\n",
    "    frame = video_capture.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_process = np.copy(frame)\n",
    "    \n",
    "#     #-------------------------------------------------------#\n",
    "#     # Face recognition\n",
    "\n",
    "#     # Convert the image from BGR color to RGB to face_recognition use\n",
    "#     rgb_frame = frame_process[:, :, ::-1]\n",
    "\n",
    "#     # Find all the faces and face encodings in the current frame of video\n",
    "#     face_locations = face_recognition.face_locations(rgb_frame)\n",
    "#     face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "#     for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "#         distance = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "#         min_distance = np.min(distance)\n",
    "#         index_point_min = np.argmin(distance)\n",
    "#         if min_distance < 0.5:\n",
    "#             name = known_face_names[index_point_min]\n",
    "#             print(name)\n",
    "#             cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 3)\n",
    "#             cv2.putText(frame, name, (left + 6, top - 6), font, 1.0, (255, 255, 0), 1)\n",
    "\n",
    "#     #-----------------------------------------------------------#\n",
    "    \n",
    "    # Detect state standing and sleeping and sitting\n",
    "    image = Image.fromarray(frame_process)\n",
    "\n",
    "    # Process detect hand and recognition furniture\n",
    "    boxed_image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
    "    image_data = np.array(boxed_image, dtype='float32')\n",
    "    \n",
    "    image_data /= 255.\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    \n",
    "    out_boxes, out_scores, out_classes = sess.run([boxes, scores, classes],\n",
    "                                                 feed_dict={\n",
    "                                                     yolo_model.input: image_data,\n",
    "                                                     input_image_shape: [image.size[1], image.size[0]],\n",
    "                                                     K.learning_phase(): 0\n",
    "                                                 })\n",
    "\n",
    "    for i, c in reversed(list(enumerate(out_classes))):\n",
    "        predicted_class = class_names[c]\n",
    "        box = out_boxes[i]\n",
    "        score = out_scores[i]\n",
    "        \n",
    "        label = '{} {:.2f}'.format(predicted_class, score)\n",
    "        \n",
    "        top, left, bottom, right = box  \n",
    "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "        \n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "        cv2.putText(frame, label, (left + 6, top + 30), font, 1.0, (255, 0, 255), 1)\n",
    "\n",
    "        \n",
    "#         #-------------------------------------------------------#\n",
    "#         # Face recognition\n",
    "#         crop_img = frame_process[top:bottom, left:right]\n",
    "#         # Convert the image from BGR color to RGB to face_recognition use\n",
    "#         rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "#         # Find all the faces and face encodings in the current frame of video\n",
    "#         face_locations = face_recognition.face_locations(rgb_frame)\n",
    "#         face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "        \n",
    "#         if not face_encodings:\n",
    "#             cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "#         else:\n",
    "#             for (top1, right1, bottom1, left1), face_encoding in zip(face_locations, face_encodings):\n",
    "#                 distance = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "#                 min_distance = np.min(distance)\n",
    "#                 index_point_min = np.argmin(distance)\n",
    "#                 if min_distance < 0.5:\n",
    "#                     name = known_face_names[index_point_min]\n",
    "#                     print(name)\n",
    "# #                     cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "#                     label = name + \": \" + label\n",
    "#                     cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "#                 else:\n",
    "#                     label = \"Unknown\" + \": \" + label\n",
    "#                     cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "# #         #-------------------------------------------------------#\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    #\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"data/image/hanoi.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_crop = img[0:800, 1000:1400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not a:\n",
    "    print(\"true\")\n",
    "else:\n",
    "    print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
