{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Tracking pose and face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use input size :\n",
    "- file cfg\n",
    "- yolo3_weight.h5 when convert\n",
    "- file train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import WebcamVideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# import cv2\n",
    "# import imutils\n",
    "# class WebcamVideoStream:\n",
    "#     def __init__(self, src=0):\n",
    "#         self.stream = cv2.VideoCapture(src)\n",
    "#         self.stream.set(3, 800)\n",
    "#         self.stream.set(4, 600)\n",
    "#         (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "#         self.stopped = False\n",
    "        \n",
    "#     def start(self):\n",
    "#         # Start the thread to read frames from the video stream\n",
    "#         Thread(target=self.update, args=()).start()\n",
    "#         return self\n",
    "    \n",
    "#     def update(self):\n",
    "#         while True:\n",
    "#             if self.stopped:\n",
    "#                 return\n",
    "            \n",
    "#             (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "#     def read(self):\n",
    "#         return self.frame\n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.stopped = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5'\n",
    "\n",
    "anchors_path = '../model_data/yolo_anchors.txt'\n",
    "classes_path = '../model_data/human_pose.txt'\n",
    "score = 0.7\n",
    "iou = 0.25\n",
    "model_image_size = (416, 416)\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class\n",
    "classes_path = os.path.expanduser(classes_path)\n",
    "with open(classes_path) as f:\n",
    "    class_names = f.readlines()\n",
    "\n",
    "class_names = [c.strip() for c in class_names]\n",
    "\n",
    "# Anchors\n",
    "anchors_path = os.path.expanduser(anchors_path)\n",
    "with open(anchors_path) as f:\n",
    "    anchors = f.readline()\n",
    "anchors = [float(x) for x in anchors.split(',')]\n",
    "anchors = np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a74128e573cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a74128e573cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_anchors\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Mismatch between model and given anchor and class sizes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = os.path.expanduser(model_path)\n",
    "assert model_path.endswith('.h5'), 'Keras model end with file .h5'\n",
    "\n",
    "num_anchors = len(anchors)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "is_tiny_version = num_anchors==6\n",
    "try:\n",
    "    yolo_model = load_model(model_path, compile=False)\n",
    "except:\n",
    "    if is_tiny_version:\n",
    "        yolo_model = tiny_yolo_body(Input(shape=(None, None, 3)), num_anchors//2, num_classes)\n",
    "    else:\n",
    "        yolo_model = yolo_body(Input(shape=(None, None, 3)), num_anchors//3, num_classes)\n",
    "    \n",
    "    yolo_model.load_weights(model_path)\n",
    "else:\n",
    "    yolo_model.layers[-1].output_shape[-1] == num_anchors/len(yolo_model.output) * (num_classes + 5), 'Mismatch between model and given anchor and class sizes'\n",
    "    \n",
    "print(\"{} model, anchors, and classes loaded.\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerpoints = []\n",
    "namefromcenterpoint = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_encodings_in_room = []\n",
    "face_names_in_room = []\n",
    "known_face_encodings_array = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "# Convert nparray -> List to face_encoding\n",
    "len_of_array_known_face_names = len(known_face_names)\n",
    "known_face_encodings_array = known_face_encodings_array.reshape(len_of_array_known_face_names, 128)\n",
    "known_face_encodings = []\n",
    "for i in range(len_of_array_known_face_names):\n",
    "    known_face_encodings.append(known_face_encodings_array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_points(centerpoints, point):\n",
    "    distance_centerpoint = [distance.euclidean(centerpoints[i], point) for i in range(len(centerpoints))]  \n",
    "    index_distance_min = np.argmin(distance_centerpoint)\n",
    "    distance_min = np.min(distance_centerpoint)\n",
    "    return index_distance_min, distance_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use face recognition\n",
    "def detect_name(frame, face_locations, face_encodings, known_face_encodings, box, label):\n",
    "    top, left, bottom, right = box\n",
    "    for (top1, right1, bottom1, left1), face_encoding in zip(face_locations, face_encodings):\n",
    "        distance = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        min_distance = np.min(distance)\n",
    "        index_point_min = np.argmin(distance)\n",
    "        if min_distance < 0.5:\n",
    "            name = known_face_names[index_point_min]\n",
    "            print(name)\n",
    "#                     cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "            label = name + \": \" + label\n",
    "            cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "        else:\n",
    "            label = \"unknown\" + \": \" + label\n",
    "            cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "    return frame, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_image_shape = K.placeholder(shape=(2, ))\n",
    "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, len(class_names), input_image_shape, score_threshold=score, iou_threshold=iou)\n",
    "num_frame = 0\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "centerpoints = []\n",
    "namefromcenterpoint = []\n",
    "\n",
    "# Video capture\n",
    "video_capture = WebcamVideoStream(src=0).start()\n",
    "\n",
    "while True:\n",
    "    num_frame += 1\n",
    "\n",
    "    # Read video frame and flip camera\n",
    "    frame = video_capture.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_process = np.copy(frame)\n",
    "        \n",
    "    # Detect state standing and sleeping and sitting\n",
    "    image = Image.fromarray(frame_process)\n",
    "\n",
    "    # Process detect hand and recognition furniture\n",
    "    boxed_image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
    "    image_data = np.array(boxed_image, dtype='float32')\n",
    "    \n",
    "    image_data /= 255.\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    \n",
    "    # Rim keras backend tensorflow forward neural network\n",
    "    out_boxes, out_scores, out_classes = sess.run([boxes, scores, classes],\n",
    "                                                 feed_dict={\n",
    "                                                     yolo_model.input: image_data,\n",
    "                                                     input_image_shape: [image.size[1], image.size[0]],\n",
    "                                                     K.learning_phase(): 0\n",
    "                                                 })\n",
    "\n",
    "    for i, c in reversed(list(enumerate(out_classes))):\n",
    "        predicted_class = class_names[c]\n",
    "        box = out_boxes[i]\n",
    "        score = out_scores[i]\n",
    "        \n",
    "        label = '{} {:.2f}'.format(predicted_class, score)\n",
    "        \n",
    "        top, left, bottom, right = box\n",
    "        print(type(top))\n",
    "        top = int(top)\n",
    "        left = int(left)\n",
    "        bottom = int(bottom)\n",
    "        right = int(right)\n",
    "    \n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)        \n",
    "    \n",
    "        # Tracking object use center point\n",
    "        x_point_new = (top + bottom) / 2.\n",
    "        y_point_new = (left + right) / 2.\n",
    "    \n",
    "        point = np.asarray([x_point_new, y_point_new])\n",
    "        \n",
    "        if centerpoints:\n",
    "            index_distance_min, distance_min = compare_points(centerpoints, point)\n",
    "\n",
    "            # Compare distance min with (bottom - top) / 4\n",
    "            if distance_min < (bottom - top) / 4.:\n",
    "                # point new same name index distance min\n",
    "                name = namefromcenterpoint[index_distance_min]        \n",
    "                label = name + \": \" + label + \"don't compute\"\n",
    "                cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "                # Update center point\n",
    "                centerpoints[index_distance_min] = point\n",
    "            else:\n",
    "                #-------------------------------------------------------#\n",
    "                # Face recognition\n",
    "                crop_img = frame_process[top:bottom, left:right]\n",
    "                # Convert the image from BGR color to RGB to face_recognition use\n",
    "                rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "                # Find all the faces and face encodings in the current frame of video\n",
    "                face_locations = face_recognition.face_locations(rgb_frame)\n",
    "                face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "                if not face_encodings:\n",
    "                    cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "                else:\n",
    "                    frame, name = detect_name(frame, face_locations, face_encodings, known_face_encodings, \n",
    "                                known_face_names, (top, left, bottom, right), label)\n",
    "                    centerpoints.append(point)\n",
    "                    namefromcenterpoint.append(name)\n",
    "        else:\n",
    "            # Face recognition\n",
    "            crop_img = frame_process[top:bottom, left:right]\n",
    "            # Convert the image from BGR color to RGB to face_recognition use\n",
    "            rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            if not face_encodings:\n",
    "                cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "            else:\n",
    "                frame, name = detect_name(frame, face_locations, face_encodings, known_face_encodings, \n",
    "                            known_face_names, (top, left, bottom, right), label)\n",
    "                centerpoints.append(point)\n",
    "                namefromcenterpoint.append(name)\n",
    "    #         #-------------------------------------------------------#        \n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    #\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking multi faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    face_encodings_in_room = []\n",
    "    face_names_in_room = []\n",
    "    known_face_encodings_array = np.load(path + \"known_face_encoding.npy\")\n",
    "    known_face_names = np.load(path + \"known_face_names.npy\")\n",
    "    \n",
    "    # Convert nparray -> list to face encoding\n",
    "    len_face = len(known_face_names)\n",
    "    known_face_encodings_array = known_face_encodings_array.reshape(len_face, 128)\n",
    "    known_face_encodings_list = []\n",
    "    for i in range(len_face):\n",
    "        known_face_encodings_list.append(known_face_encodings_array[i])\n",
    "    \n",
    "    return known_face_encodings_list, known_face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import cv2\n",
    "import imutils\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src=0):\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        self.stream.set(3, 800)\n",
    "        self.stream.set(4, 600)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "        self.stopped = False\n",
    "        \n",
    "    def start(self):\n",
    "        # Start the thread to read frames from the video stream\n",
    "        Thread(target=self.update, args=()).start()\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.stopped:\n",
    "                return\n",
    "            \n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "    def read(self):\n",
    "        return self.frame\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_ecodings_list, known_face_names = load_data(\"../data/numpy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.MultiTracker_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add',\n",
       " 'clear',\n",
       " 'create',\n",
       " 'empty',\n",
       " 'getDefaultName',\n",
       " 'getObjects',\n",
       " 'read',\n",
       " 'save',\n",
       " 'update',\n",
       " 'write']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use face recognition  0\n",
      "Use face recognition  10\n",
      "Use face recognition  20\n",
      "Use face recognition  30\n",
      "Use face recognition  40\n",
      "Use face recognition  50\n",
      "Use face recognition  60\n",
      "Use face recognition  70\n",
      "Use face recognition  80\n",
      "Use face recognition  90\n",
      "Use face recognition  100\n",
      "Use face recognition  110\n",
      "Use face recognition  120\n",
      "Use face recognition  130\n",
      "Use face recognition  140\n",
      "Use face recognition  150\n",
      "Use face recognition  160\n",
      "Use face recognition  170\n",
      "Use face recognition  180\n",
      "Use face recognition  190\n",
      "Use face recognition  200\n",
      "Use face recognition  210\n",
      "Use face recognition  220\n",
      "Use face recognition  230\n",
      "Use face recognition  240\n",
      "Use face recognition  250\n",
      "Use face recognition  260\n",
      "Use face recognition  270\n",
      "Use face recognition  280\n",
      "Use face recognition  290\n",
      "Use face recognition  300\n",
      "Use face recognition  310\n",
      "Use face recognition  320\n",
      "Use face recognition  330\n",
      "Use face recognition  340\n",
      "Use face recognition  350\n",
      "Use face recognition  360\n",
      "Use face recognition  370\n",
      "Use face recognition  380\n",
      "Use face recognition  390\n",
      "Use face recognition  400\n",
      "Use face recognition  410\n",
      "Use face recognition  420\n",
      "Use face recognition  430\n",
      "Use face recognition  440\n",
      "Use face recognition  450\n",
      "Use face recognition  460\n",
      "Use face recognition  470\n",
      "Use face recognition  480\n",
      "Use face recognition  490\n",
      "Use face recognition  500\n",
      "Use face recognition  510\n",
      "Use face recognition  520\n",
      "Use face recognition  530\n",
      "Use face recognition  540\n",
      "Use face recognition  550\n",
      "Use face recognition  560\n",
      "Use face recognition  570\n",
      "Use face recognition  580\n",
      "Use face recognition  590\n",
      "Use face recognition  600\n",
      "Use face recognition  610\n",
      "Use face recognition  620\n",
      "Use face recognition  630\n",
      "Use face recognition  640\n",
      "Use face recognition  650\n",
      "Use face recognition  660\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dfd28a896ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     rgb_frame = frame[:, :, ::-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mframe_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mface_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_capture = WebcamVideoStream(src=0).start()\n",
    "number_frame = 0\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "tracker = cv2.MultiTracker_create()\n",
    "init_once = False\n",
    "name_boxes = []\n",
    "\n",
    "while True:\n",
    "    frame = video_capture.read()\n",
    "#     rgb_frame = frame[:, :, ::-1]\n",
    "    frame_process = frame\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "    \n",
    "    if (number_frame % 10 == 0):\n",
    "        tracker.clear()\n",
    "        name_boxes = []\n",
    "        tracker = cv2.MultiTracker_create()\n",
    "        print(\"Use face recognition \", number_frame)\n",
    "        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "            if len(known_face_ecodings_list) > 0:\n",
    "                distance = face_recognition.face_distance(known_face_ecodings_list, face_encoding)\n",
    "                point = np.min(distance)\n",
    "                index_point_min = np.argmin(distance)\n",
    "#                 print(top, right, bottom, left)\n",
    "                # left top, \n",
    "#                 ok = tracker.add(cv2.TrackerMIL_create(), frame, (139, 510, 325, 324))\n",
    "                ok = tracker.add(cv2.TrackerMIL_create(), frame, (left, top, (right-left), (bottom-top)))\n",
    "                if point > 0.4:\n",
    "                    name = \"Unknown\"\n",
    "                    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 3)\n",
    "                    cv2.putText(frame, name, (left + 6, top - 6), font, 1.0, (0, 0, 255), 1)\n",
    "                else:\n",
    "                    name = known_face_names[index_point_min]\n",
    "                    cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "                    cv2.putText(frame, name, (left + 6, top - 6), font, 1.0, (0, 0, 255), 1)\n",
    "                name_boxes.append(name)\n",
    "    ok, boxes = tracker.update(frame)\n",
    "    for newbox, name in zip(boxes, name_boxes):\n",
    "        p1 = (int(newbox[0]), int(newbox[1]))\n",
    "        p2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 0, 255))\n",
    "        cv2.putText(frame, name, p1, font, 1.0, (0, 0, 255), 1)\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "               \n",
    "    number_frame += 1\n",
    "               \n",
    "    # Press Q on keyboard to stop recording\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = (1, 2, 3, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
