{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Tracking pose and face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use input size :\n",
    "- file cfg\n",
    "- yolo3_weight.h5 when convert\n",
    "- file train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import WebcamVideoStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# import cv2\n",
    "# import imutils\n",
    "# class WebcamVideoStream:\n",
    "#     def __init__(self, src=0):\n",
    "#         self.stream = cv2.VideoCapture(src)\n",
    "#         self.stream.set(3, 800)\n",
    "#         self.stream.set(4, 600)\n",
    "#         (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "#         self.stopped = False\n",
    "        \n",
    "#     def start(self):\n",
    "#         # Start the thread to read frames from the video stream\n",
    "#         Thread(target=self.update, args=()).start()\n",
    "#         return self\n",
    "    \n",
    "#     def update(self):\n",
    "#         while True:\n",
    "#             if self.stopped:\n",
    "#                 return\n",
    "            \n",
    "#             (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "#     def read(self):\n",
    "#         return self.frame\n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.stopped = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5'\n",
    "\n",
    "anchors_path = '../model_data/yolo_anchors.txt'\n",
    "classes_path = '../model_data/human_pose.txt'\n",
    "score = 0.7\n",
    "iou = 0.25\n",
    "model_image_size = (416, 416)\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class\n",
    "classes_path = os.path.expanduser(classes_path)\n",
    "with open(classes_path) as f:\n",
    "    class_names = f.readlines()\n",
    "\n",
    "class_names = [c.strip() for c in class_names]\n",
    "\n",
    "# Anchors\n",
    "anchors_path = os.path.expanduser(anchors_path)\n",
    "with open(anchors_path) as f:\n",
    "    anchors = f.readline()\n",
    "anchors = [float(x) for x in anchors.split(',')]\n",
    "anchors = np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a74128e573cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a74128e573cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_anchors\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Mismatch between model and given anchor and class sizes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '../logs/human_pose_dataset_1400_416_yolo/trained_weights_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = os.path.expanduser(model_path)\n",
    "assert model_path.endswith('.h5'), 'Keras model end with file .h5'\n",
    "\n",
    "num_anchors = len(anchors)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "is_tiny_version = num_anchors==6\n",
    "try:\n",
    "    yolo_model = load_model(model_path, compile=False)\n",
    "except:\n",
    "    if is_tiny_version:\n",
    "        yolo_model = tiny_yolo_body(Input(shape=(None, None, 3)), num_anchors//2, num_classes)\n",
    "    else:\n",
    "        yolo_model = yolo_body(Input(shape=(None, None, 3)), num_anchors//3, num_classes)\n",
    "    \n",
    "    yolo_model.load_weights(model_path)\n",
    "else:\n",
    "    yolo_model.layers[-1].output_shape[-1] == num_anchors/len(yolo_model.output) * (num_classes + 5), 'Mismatch between model and given anchor and class sizes'\n",
    "    \n",
    "print(\"{} model, anchors, and classes loaded.\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerpoints = []\n",
    "namefromcenterpoint = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_encodings_in_room = []\n",
    "face_names_in_room = []\n",
    "known_face_encodings_array = np.load(\"../data/numpy/known_face_encoding.npy\")\n",
    "known_face_names = np.load(\"../data/numpy/known_face_names.npy\")\n",
    "\n",
    "# Convert nparray -> List to face_encoding\n",
    "len_of_array_known_face_names = len(known_face_names)\n",
    "known_face_encodings_array = known_face_encodings_array.reshape(len_of_array_known_face_names, 128)\n",
    "known_face_encodings = []\n",
    "for i in range(len_of_array_known_face_names):\n",
    "    known_face_encodings.append(known_face_encodings_array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_points(centerpoints, point):\n",
    "    distance_centerpoint = [distance.euclidean(centerpoints[i], point) for i in range(len(centerpoints))]  \n",
    "    index_distance_min = np.argmin(distance_centerpoint)\n",
    "    distance_min = np.min(distance_centerpoint)\n",
    "    return index_distance_min, distance_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use face recognition\n",
    "def detect_name(frame, face_locations, face_encodings, known_face_encodings, box, label):\n",
    "    top, left, bottom, right = box\n",
    "    for (top1, right1, bottom1, left1), face_encoding in zip(face_locations, face_encodings):\n",
    "        distance = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        min_distance = np.min(distance)\n",
    "        index_point_min = np.argmin(distance)\n",
    "        if min_distance < 0.5:\n",
    "            name = known_face_names[index_point_min]\n",
    "            print(name)\n",
    "#                     cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "            label = name + \": \" + label\n",
    "            cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "        else:\n",
    "            label = \"unknown\" + \": \" + label\n",
    "            cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "    return frame, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_image_shape = K.placeholder(shape=(2, ))\n",
    "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, len(class_names), input_image_shape, score_threshold=score, iou_threshold=iou)\n",
    "num_frame = 0\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "centerpoints = []\n",
    "namefromcenterpoint = []\n",
    "\n",
    "# Video capture\n",
    "video_capture = WebcamVideoStream(src=0).start()\n",
    "\n",
    "while True:\n",
    "    num_frame += 1\n",
    "\n",
    "    # Read video frame and flip camera\n",
    "    frame = video_capture.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_process = np.copy(frame)\n",
    "        \n",
    "    # Detect state standing and sleeping and sitting\n",
    "    image = Image.fromarray(frame_process)\n",
    "\n",
    "    # Process detect hand and recognition furniture\n",
    "    boxed_image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
    "    image_data = np.array(boxed_image, dtype='float32')\n",
    "    \n",
    "    image_data /= 255.\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    \n",
    "    # Rim keras backend tensorflow forward neural network\n",
    "    out_boxes, out_scores, out_classes = sess.run([boxes, scores, classes],\n",
    "                                                 feed_dict={\n",
    "                                                     yolo_model.input: image_data,\n",
    "                                                     input_image_shape: [image.size[1], image.size[0]],\n",
    "                                                     K.learning_phase(): 0\n",
    "                                                 })\n",
    "\n",
    "    for i, c in reversed(list(enumerate(out_classes))):\n",
    "        predicted_class = class_names[c]\n",
    "        box = out_boxes[i]\n",
    "        score = out_scores[i]\n",
    "        \n",
    "        label = '{} {:.2f}'.format(predicted_class, score)\n",
    "        \n",
    "        top, left, bottom, right = box\n",
    "        print(type(top))\n",
    "        top = int(top)\n",
    "        left = int(left)\n",
    "        bottom = int(bottom)\n",
    "        right = int(right)\n",
    "    \n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)        \n",
    "    \n",
    "        # Tracking object use center point\n",
    "        x_point_new = (top + bottom) / 2.\n",
    "        y_point_new = (left + right) / 2.\n",
    "    \n",
    "        point = np.asarray([x_point_new, y_point_new])\n",
    "        \n",
    "        if centerpoints:\n",
    "            index_distance_min, distance_min = compare_points(centerpoints, point)\n",
    "\n",
    "            # Compare distance min with (bottom - top) / 4\n",
    "            if distance_min < (bottom - top) / 4.:\n",
    "                # point new same name index distance min\n",
    "                name = namefromcenterpoint[index_distance_min]        \n",
    "                label = name + \": \" + label + \"don't compute\"\n",
    "                cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "                # Update center point\n",
    "                centerpoints[index_distance_min] = point\n",
    "            else:\n",
    "                #-------------------------------------------------------#\n",
    "                # Face recognition\n",
    "                crop_img = frame_process[top:bottom, left:right]\n",
    "                # Convert the image from BGR color to RGB to face_recognition use\n",
    "                rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "                # Find all the faces and face encodings in the current frame of video\n",
    "                face_locations = face_recognition.face_locations(rgb_frame)\n",
    "                face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "                if not face_encodings:\n",
    "                    cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "                else:\n",
    "                    frame, name = detect_name(frame, face_locations, face_encodings, known_face_encodings, \n",
    "                                known_face_names, (top, left, bottom, right), label)\n",
    "                    centerpoints.append(point)\n",
    "                    namefromcenterpoint.append(name)\n",
    "        else:\n",
    "            # Face recognition\n",
    "            crop_img = frame_process[top:bottom, left:right]\n",
    "            # Convert the image from BGR color to RGB to face_recognition use\n",
    "            rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            if not face_encodings:\n",
    "                cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "            else:\n",
    "                frame, name = detect_name(frame, face_locations, face_encodings, known_face_encodings, \n",
    "                            known_face_names, (top, left, bottom, right), label)\n",
    "                centerpoints.append(point)\n",
    "                namefromcenterpoint.append(name)\n",
    "    #         #-------------------------------------------------------#        \n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    #\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking multi faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    face_encodings_in_room = []\n",
    "    face_names_in_room = []\n",
    "    known_face_encodings_array = np.load(path + \"known_face_encoding.npy\")\n",
    "    known_face_names = np.load(path + \"known_face_names.npy\")\n",
    "    \n",
    "    # Convert nparray -> list to face encoding\n",
    "    len_face = len(known_face_names)\n",
    "    known_face_encodings_array = known_face_encodings_array.reshape(len_face, 128)\n",
    "    known_face_encodings_list = []\n",
    "    for i in range(len_face):\n",
    "        known_face_encodings_list.append(known_face_encodings_array[i])\n",
    "    \n",
    "    return known_face_encodings_list, known_face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import cv2\n",
    "import imutils\n",
    "class WebcamVideoStream:\n",
    "    def __init__(self, src=0):\n",
    "        self.stream = cv2.VideoCapture(src)\n",
    "        self.stream.set(3, 800)\n",
    "        self.stream.set(4, 600)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "        self.stopped = False\n",
    "        \n",
    "    def start(self):\n",
    "        # Start the thread to read frames from the video stream\n",
    "        Thread(target=self.update, args=()).start()\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.stopped:\n",
    "                return\n",
    "            \n",
    "            (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "    def read(self):\n",
    "        return self.frame\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_ecodings_list, known_face_names = load_data(\"../data/numpy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.MultiTracker_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'add',\n",
       " 'clear',\n",
       " 'create',\n",
       " 'empty',\n",
       " 'getDefaultName',\n",
       " 'getObjects',\n",
       " 'read',\n",
       " 'save',\n",
       " 'update',\n",
       " 'write']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_area(temp_box, list_box):\n",
    "    dx = min(a.xmax,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "name boxes 1 []\n",
      "[]\n",
      "name boxes 2 []\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "()\n",
      "name boxes 1 []\n",
      "[]\n",
      "name boxes 2 []\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "<MultiTracker 0x7fc8647c3f90>\n",
      "()\n",
      "name boxes 1 []\n",
      "[]\n",
      "name boxes 2 []\n",
      "Dam Van Tai\n",
      "name_boxes 3 []\n",
      "name_boxes 4 ['Dam Van Tai']\n",
      "name_boxes 5 ['Dam Van Tai']\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[448. 139. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[424. 145. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[432. 146. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[429. 144. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[423. 145. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[422. 146. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[422. 148. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[420. 149. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[420. 149. 186. 186.]]\n",
      "<MultiTracker 0x7fc8647c3ed0>\n",
      "[[419. 149. 186. 186.]]\n",
      "name boxes 1 ['Dam Van Tai']\n",
      "['Dam Van Tai']\n",
      "name boxes 2 ['Dam Van Tai']\n",
      "Dam Van Tai\n",
      "name_boxes 3 ['Dam Van Tai']\n",
      "name_boxes 4 ['Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 5 ['Dam Van Tai', 'Dam Van Tai']\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[420. 145. 186. 186.]\n",
      " [428. 140. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[418. 149. 186. 186.]\n",
      " [426. 140. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[417. 150. 186. 186.]\n",
      " [425. 141. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[417. 150. 186. 186.]\n",
      " [425. 142. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[417. 151. 186. 186.]\n",
      " [426. 143. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[416. 152. 186. 186.]\n",
      " [424. 144. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[417. 152. 186. 186.]\n",
      " [425. 144. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[416. 153. 186. 186.]\n",
      " [425. 144. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[417. 154. 186. 186.]\n",
      " [425. 145. 185. 186.]]\n",
      "<MultiTracker 0x7fc8647c3fb0>\n",
      "[[416. 156. 186. 186.]\n",
      " [424. 147. 185. 186.]]\n",
      "name boxes 1 ['Dam Van Tai', 'Dam Van Tai']\n",
      "['Dam Van Tai', 'Dam Van Tai']\n",
      "name boxes 2 ['Dam Van Tai', 'Dam Van Tai']\n",
      "Dam Van Tai\n",
      "name_boxes 3 ['Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 4 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 5 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[416. 156. 186. 186.]\n",
      " [424. 148. 185. 186.]\n",
      " [390. 118. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[411. 160. 186. 186.]\n",
      " [418. 151. 185. 186.]\n",
      " [384. 121. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[388. 162. 186. 186.]\n",
      " [395. 160. 185. 186.]\n",
      " [360. 127. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[386. 173. 186. 186.]\n",
      " [388. 173. 185. 186.]\n",
      " [351. 139. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[369. 181. 186. 186.]\n",
      " [369. 183. 185. 186.]\n",
      " [332. 146. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[358. 203. 186. 186.]\n",
      " [361. 197. 185. 186.]\n",
      " [314. 156. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[351. 202. 186. 186.]\n",
      " [348. 218. 185. 186.]\n",
      " [310. 155. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[351. 205. 186. 186.]\n",
      " [352. 209. 185. 186.]\n",
      " [312. 148. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[372. 192. 186. 186.]\n",
      " [368. 190. 185. 186.]\n",
      " [329. 153. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3e70>\n",
      "[[396. 188. 186. 186.]\n",
      " [369. 214. 185. 186.]\n",
      " [323. 149. 223. 223.]]\n",
      "name boxes 1 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name boxes 2 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "Dam Van Tai\n",
      "name_boxes 3 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 4 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 5 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[402. 183. 186. 186.]\n",
      " [364. 210. 185. 186.]\n",
      " [322. 148. 223. 223.]\n",
      " [413. 142. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[381. 196. 186. 186.]\n",
      " [367. 212. 185. 186.]\n",
      " [325. 135. 223. 223.]\n",
      " [419. 144. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[361. 210. 186. 186.]\n",
      " [355. 227. 185. 186.]\n",
      " [302. 144. 223. 223.]\n",
      " [396. 136. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[373. 218. 186. 186.]\n",
      " [373. 227. 185. 186.]\n",
      " [304. 143. 223. 223.]\n",
      " [407. 136. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[374. 221. 186. 186.]\n",
      " [371. 235. 185. 186.]\n",
      " [302. 144. 223. 223.]\n",
      " [418. 140. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[367. 219. 186. 186.]\n",
      " [359. 230. 185. 186.]\n",
      " [300. 145. 223. 223.]\n",
      " [411. 139. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[366. 219. 186. 186.]\n",
      " [359. 236. 185. 186.]\n",
      " [299. 146. 223. 223.]\n",
      " [409. 138. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[362. 216. 186. 186.]\n",
      " [357. 232. 185. 186.]\n",
      " [294. 147. 223. 223.]\n",
      " [404. 136. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[360. 215. 186. 186.]\n",
      " [357. 232. 185. 186.]\n",
      " [295. 145. 223. 223.]\n",
      " [403. 135. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3fd0>\n",
      "[[362. 211. 186. 186.]\n",
      " [357. 230. 185. 186.]\n",
      " [295. 141. 223. 223.]\n",
      " [403. 134. 223. 223.]]\n",
      "name boxes 1 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name boxes 2 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "Dam Van Tai\n",
      "name_boxes 3 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 4 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name_boxes 5 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[352. 211. 186. 186.]\n",
      " [355. 232. 185. 186.]\n",
      " [294. 141. 223. 223.]\n",
      " [409. 147. 223. 223.]\n",
      " [315. 142. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[376. 216. 186. 186.]\n",
      " [378. 234. 185. 186.]\n",
      " [317. 146. 223. 223.]\n",
      " [423. 137. 223. 223.]\n",
      " [333. 142. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[372. 195. 186. 186.]\n",
      " [377. 210. 185. 186.]\n",
      " [317. 136. 223. 223.]\n",
      " [409. 121. 223. 223.]\n",
      " [326. 129. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[386. 200. 186. 186.]\n",
      " [400. 201. 185. 186.]\n",
      " [341. 130. 223. 223.]\n",
      " [405. 103. 223. 223.]\n",
      " [349. 120. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[404. 197. 186. 186.]\n",
      " [404. 205. 185. 186.]\n",
      " [342. 139. 223. 223.]\n",
      " [413. 106. 223. 223.]\n",
      " [356. 121. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[402. 206. 186. 186.]\n",
      " [396. 206. 185. 186.]\n",
      " [334. 142. 223. 223.]\n",
      " [432.  95. 223. 223.]\n",
      " [343. 142. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[403. 204. 186. 186.]\n",
      " [403. 206. 185. 186.]\n",
      " [334. 143. 223. 223.]\n",
      " [433.  95. 223. 223.]\n",
      " [342. 141. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[402. 204. 186. 186.]\n",
      " [404. 205. 185. 186.]\n",
      " [334. 142. 223. 223.]\n",
      " [433.  92. 223. 223.]\n",
      " [342. 140. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[400. 203. 186. 186.]\n",
      " [403. 204. 185. 186.]\n",
      " [335. 142. 223. 223.]\n",
      " [434.  92. 223. 223.]\n",
      " [344. 139. 223. 223.]]\n",
      "<MultiTracker 0x7fc8647c3f70>\n",
      "[[394. 208. 186. 186.]\n",
      " [398. 208. 185. 186.]\n",
      " [332. 143. 223. 223.]\n",
      " [430.  91. 223. 223.]\n",
      " [340. 141. 223. 223.]]\n",
      "name boxes 1 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "name boxes 2 ['Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai', 'Dam Van Tai']\n",
      "<MultiTracker 0x7fc86461d0f0>\n",
      "[[392. 200. 186. 186.]\n",
      " [396. 204. 185. 186.]\n",
      " [333. 147. 223. 223.]\n",
      " [430.  96. 223. 223.]\n",
      " [333. 147. 223. 223.]]\n",
      "<MultiTracker 0x7fc86461d0f0>\n",
      "[[398. 213. 186. 186.]\n",
      " [407. 226. 185. 186.]\n",
      " [341. 155. 223. 223.]\n",
      " [408.  98. 223. 223.]\n",
      " [315. 164. 223. 223.]]\n",
      "<MultiTracker 0x7fc86461d0f0>\n",
      "[[407. 205. 186. 186.]\n",
      " [411. 217. 185. 186.]\n",
      " [347. 149. 223. 223.]\n",
      " [414.  90. 223. 223.]\n",
      " [323. 153. 223. 223.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MultiTracker 0x7fc86461d0f0>\n",
      "[[422. 217. 186. 186.]\n",
      " [410. 226. 185. 186.]\n",
      " [371. 154. 223. 223.]\n",
      " [438.  88. 223. 223.]\n",
      " [344. 159. 223. 223.]]\n",
      "<MultiTracker 0x7fc86461d0f0>\n",
      "[[441. 201. 186. 186.]\n",
      " [430. 226. 185. 186.]\n",
      " [380. 131. 223. 223.]\n",
      " [446.  65. 223. 223.]\n",
      " [362. 142. 223. 223.]]\n"
     ]
    }
   ],
   "source": [
    "video_capture = WebcamVideoStream(src=0).start()\n",
    "number_frame = 0\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "tracker = cv2.MultiTracker_create()\n",
    "init_once = False\n",
    "name_boxes = []\n",
    "boxes_known = []\n",
    "name_known = []\n",
    "while True:\n",
    "    frame = video_capture.read()\n",
    "#     rgb_frame = frame[:, :, ::-1]\n",
    "    frame_process = frame\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "    \n",
    "    if (number_frame % 10 == 0):\n",
    "        # Delete tracker and initi tracker\n",
    "        tracker.clear()\n",
    "        tracker = cv2.MultiTracker_create()\n",
    "#         name_boxes = []\n",
    "        if init_once == True:\n",
    "            # Create tracker with name known from before frame, and init name_boxes with people known\n",
    "            #\n",
    "            name_boxes_temp = [] # name of people \n",
    "            boxes_known = [] # box of people known\n",
    "            print(\"name boxes 1\", name_boxes)\n",
    "            for newbow, name in zip(boxes, name_boxes):\n",
    "                if name != \"unknown\":\n",
    "                    ok = tracker.add(cv2.TrackerMIL_create(), frame, (newbow[0], newbow[1] , newbow[2], newbow[3]))\n",
    "                    name_boxes_temp.append(name)\n",
    "                    boxes_known.append(newbow)\n",
    "            print(name_boxes_temp)\n",
    "        # With people known, continue face recognition in next frame\n",
    "        # And only change name unknown if (area between unknown and predict > 90 %)\n",
    "            name_boxes = name_boxes_temp\n",
    "            print(\"name boxes 2\", name_boxes)\n",
    "        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "            if len(known_face_ecodings_list) > 0:\n",
    "                distance = face_recognition.face_distance(known_face_ecodings_list, face_encoding)\n",
    "                point = np.min(distance)\n",
    "                index_point_min = np.argmin(distance)\n",
    "#                 print(top, right, bottom, left)\n",
    "                # left top, \n",
    "#                 ok = tracker.add(cv2.TrackerMIL_create(), frame, (139, 510, 325, 324))\n",
    "                temp_box = (top, left, bottom, right)\n",
    "                if point > 0.4:\n",
    "                    name = \"Unknown\"\n",
    "                    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 3)\n",
    "                    cv2.putText(frame, name, (left + 6, top - 6), font, 1.0, (0, 0, 255), 1)\n",
    "                elif point <= 0.4:\n",
    "                    name = known_face_names[index_point_min]\n",
    "                    print(name)\n",
    "                    print(\"name_boxes 3\", name_boxes)\n",
    "                    if name in not name_boxes:\n",
    "                        ok = tracker.add(cv2.TrackerMIL_create(), frame, (left, top, (right-left), (bottom-top)))\n",
    "                        cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "                        cv2.putText(frame, name, (left + 6, top - 6), font, 1.0, (0, 0, 255), 1)\n",
    "#                     boxes_known.append((left, top, (right-left), ()))\n",
    "                        name_boxes.append(name)\n",
    "                        print(\"name_boxes 4\", name_boxes)\n",
    "                    print(\"name_boxes 5\", name_boxes)\n",
    "    ok, boxes = tracker.update(frame)\n",
    "    print(tracker)\n",
    "    print(boxes)\n",
    "    init_once = True\n",
    "    for newbox, name in zip(boxes, name_boxes):\n",
    "        p1 = (int(newbox[0]), int(newbox[1]))\n",
    "        p2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 0, 255))\n",
    "        cv2.putText(frame, name, p1, font, 1.0, (0, 0, 255), 1)\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "               \n",
    "    number_frame += 1\n",
    "               \n",
    "    # Press Q on keyboard to stop recording\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    if i == 6:\n",
    "        b.remove(i)\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
