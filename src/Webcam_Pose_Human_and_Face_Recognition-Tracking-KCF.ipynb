{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Tracking pose and face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use input size :\n",
    "- file cfg\n",
    "- yolo3_weight.h5 when convert\n",
    "- file train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import cv2\n",
    "import dlib\n",
    "import face_recognition\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "from imutils.video import WebcamVideoStream\n",
    "from imutils.video import FPS\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../model_data/yolo_weights.h5'\n",
    "anchors_path = '../model_data/yolo_anchors.txt'\n",
    "classes_path = '../model_data/coco_classes.txt'\n",
    "score = 0.4\n",
    "iou = 0.35\n",
    "model_image_size = (416, 416)\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class\n",
    "classes_path = os.path.expanduser(classes_path)\n",
    "with open(classes_path) as f:\n",
    "    class_names = f.readlines()\n",
    "\n",
    "class_names = [c.strip() for c in class_names]\n",
    "\n",
    "# Anchors\n",
    "anchors_path = os.path.expanduser(anchors_path)\n",
    "with open(anchors_path) as f:\n",
    "    anchors = f.readline()\n",
    "anchors = [float(x) for x in anchors.split(',')]\n",
    "anchors = np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model_data/yolo_weights.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_path = os.path.expanduser(model_path)\n",
    "assert model_path.endswith('.h5'), 'Keras model end with file .h5'\n",
    "\n",
    "num_anchors = len(anchors)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "is_tiny_version = num_anchors==6\n",
    "try:\n",
    "    yolo_model = load_model(model_path, compile=False)\n",
    "except:\n",
    "    if is_tiny_version:\n",
    "        yolo_model = tiny_yolo_body(Input(shape=(None, None, 3)), num_anchors//2, num_classes)\n",
    "    else:\n",
    "        yolo_model = yolo_body(Input(shape=(None, None, 3)), num_anchors//3, num_classes)\n",
    "    \n",
    "    yolo_model.load_weights(model_path)\n",
    "else:\n",
    "    yolo_model.layers[-1].output_shape[-1] == num_anchors/len(yolo_model.output) * (num_classes + 5), 'Mismatch between model and given anchor and class sizes'\n",
    "    \n",
    "print(\"{} model, anchors, and classes loaded.\".format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENCV_OBJECT_TRACKERS = {\n",
    "#     'crst': cv2.TrackerCSRT_create,\n",
    "#     'kcf': cv2.TrackerKCF_create,\n",
    "#     'boosting': cv2.TrackerBoosting_create,\n",
    "#     'mil': cv2.TrackerMIL_create,\n",
    "#     'tld': cv2.TrackerTLD_create,\n",
    "#     'medianflow': cv2.TrackerMedianFlow_create,\n",
    "#     'mosse': cv2.TrackerMOSSE_create\n",
    "# }\n",
    "# tracker = OPENCV_OBJECT_TRACKERS['kcf']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = cv2.MultiTracker_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'>\n",
      "run yolov3\n",
      "180 25 245 90\n",
      "<class 'numpy.float32'>\n",
      "run yolov3\n",
      "98 -1 630 477\n",
      "True\n",
      "[[180.  25.  65.  65.]\n",
      " [ 98.  -1. 532. 478.]]\n",
      "True\n",
      "[[174.  26.  65.  65.]\n",
      " [ 98.  -1. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 92.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 86. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 86. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 84. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 84. -13. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 70. -19. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 58. -23. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 64. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 64. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 66. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 68. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 70. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 62. -23. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 48. -27. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 42. -29. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 46. -27. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 48. -25. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 48. -25. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 48. -25. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 48. -25. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 58. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [102.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [118.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [130.  11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [126.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [126.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [124.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [120.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [120.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [122.   7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [118.   5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [120.   3. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [120.  -1. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [106. -13. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.   1. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [106.  -1. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 98.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [106.  -3. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 94. -13. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 86. -23. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 90. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 96. -17. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 96. -17. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 92. -21. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 98. -15. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 98. -13. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [114.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [114.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [114.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [106. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [114.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -5. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [112.  -7. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [110.  -9. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [108. -11. 532. 478.]]\n",
      "False\n",
      "[[174.  26.  65.  65.]\n",
      " [ 90. -17. 532. 478.]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7d2c97d6a2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFPS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitBB\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_image_shape = K.placeholder(shape=(2, ))\n",
    "boxes, scores, classes = yolo_eval(yolo_model.output, anchors, len(class_names), input_image_shape, score_threshold=score, iou_threshold=iou)\n",
    "num_frame = 0\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "initBB = None\n",
    "fps = None\n",
    "\n",
    "# Video capture\n",
    "video_capture = WebcamVideoStream(src=0).start()\n",
    "\n",
    "while True:\n",
    "    num_frame += 1\n",
    "\n",
    "    # Read video frame and flip camera\n",
    "    frame = video_capture.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    (H, W) = frame.shape[:2]\n",
    "#     frame_process = np.copy(frame)\n",
    "    fps = FPS().start()\n",
    "    if initBB is not None:\n",
    "        (success, boxes) = tracker.update(frame)\n",
    "        print(success)\n",
    "        print(boxes)\n",
    "        if success:\n",
    "            for newbox in boxes:\n",
    "                p1 = (int(newbox[0]), int(newbox[1]))\n",
    "                p2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "                cv2.rectangle(frame, p1, p2, (200, 0, 0))\n",
    "#             (x, y, w, h) = [int(v) for v in box]\n",
    "#             cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
    "#             point = np.asarray([x + w/2. , y + h/2.])\n",
    "#             center_points.append(point)\n",
    "\n",
    "        fps.update()\n",
    "        fps.stop()\n",
    "        \n",
    "        info = [\n",
    "            (\"Tracker\", (str(tracker)).split(\" \")[0] + \">\"),\n",
    "            (\"Success\", \"Yes\" if success else \"NO\"),\n",
    "            (\"FPS\", \"{:2f}\".format(fps.fps())),\n",
    "        ]\n",
    "    \n",
    "        for (i, (k, v)) in enumerate(info):\n",
    "            text = \"{}: {}\".format(k, v)\n",
    "            cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "    \n",
    "    if initBB is None:\n",
    "        image = Image.fromarray(frame)\n",
    "\n",
    "        # Process detect pe\n",
    "        boxed_image = letterbox_image(image, tuple(reversed(model_image_size)))\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)\n",
    "\n",
    "        # Rim keras backend tensorflow forward neural network\n",
    "        out_boxes, out_scores, out_classes = sess.run([boxes, scores, classes],\n",
    "                                                     feed_dict={\n",
    "                                                         yolo_model.input: image_data,\n",
    "                                                         input_image_shape: [image.size[1], image.size[0]],\n",
    "                                                         K.learning_phase(): 0\n",
    "                                                     })\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "            if predicted_class == \"person\":\n",
    "                label = '{} {:.2f}'.format(predicted_class, score)\n",
    "                top, left, bottom, right = box\n",
    "                print(type(top))\n",
    "                top = int(top)\n",
    "                left = int(left)\n",
    "                bottom = int(bottom)\n",
    "                right = int(right)\n",
    "                print(\"run yolov3\")\n",
    "#                 cv2.rectangle(frame, (left, top), (right, bottom), (255, 0, 0), 3)\n",
    "                bbox = (left, top, right - left, bottom - top)\n",
    "                tracker.add(cv2.TrackerKCF_create(), frame, bbox)\n",
    "                print(left, top, right, bottom)\n",
    "                initBB = 1\n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if centerpoints:\n",
    "#             index_distance_min, distance_min = compare_points(centerpoints, point)\n",
    "\n",
    "#             # Compare distance min with (bottom - top) / 4\n",
    "#             if distance_min < (bottom - top) / 4.:\n",
    "#                 # point new same name index distance min\n",
    "#                 name = namefromcenterpoint[index_distance_min]        \n",
    "#                 label = name + \": \" + label + \"don't compute\"\n",
    "#                 cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "#                 # Update center point\n",
    "#                 centerpoints[index_distance_min] = point\n",
    "#             else:\n",
    "#                 #-------------------------------------------------------#\n",
    "#                 # Face recognition\n",
    "#                 crop_img = frame_process[top:bottom, left:right]\n",
    "#                 # Convert the image from BGR color to RGB to face_recognition use\n",
    "#                 rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "#                 # Find all the faces and face encodings in the current frame of video\n",
    "#                 face_locations = face_recognition.face_locations(rgb_frame)\n",
    "#                 face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "#                 if not face_encodings:\n",
    "#                     cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "#                 else:\n",
    "#                     frame, name = detect_name(frame, face_locations, face_encodings, known_face_encodings, \n",
    "#                                 known_face_names, (top, left, bottom, right), label)\n",
    "#                     centerpoints.append(point)\n",
    "#                     namefromcenterpoint.append(name)\n",
    "#         else:\n",
    "#             # Face recognition\n",
    "#             crop_img = frame_process[top:bottom, left:right]\n",
    "#             # Convert the image from BGR color to RGB to face_recognition use\n",
    "#             rgb_frame = crop_img[:, :, ::-1]\n",
    "\n",
    "#             # Find all the faces and face encodings in the current frame of video\n",
    "#             face_locations = face_recognition.face_locations(rgb_frame)\n",
    "#             face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "#             if not face_encodings:\n",
    "#                 cv2.putText(frame, label, (left + 6, top + 20), font, 1.0, (0, 0, 255), 1)\n",
    "#             else:\n",
    "#                 frame, name = detect_name(frame, face_locations, face_encodings, known_face_encodings, \n",
    "#                             known_face_names, (top, left, bottom, right), label)\n",
    "#                 centerpoints.append(point)\n",
    "#                 namefromcenterpoint.append(name)\n",
    "    #         #-------------------------------------------------------#        \n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    #\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from threading import Thread\n",
    "# import cv2\n",
    "# import imutils\n",
    "# class WebcamVideoStream:\n",
    "#     def __init__(self, src=0):\n",
    "#         self.stream = cv2.VideoCapture(src)\n",
    "#         self.stream.set(3, 800)\n",
    "#         self.stream.set(4, 600)\n",
    "#         (self.grabbed, self.frame) = self.stream.read()\n",
    "        \n",
    "#         self.stopped = False\n",
    "        \n",
    "#     def start(self):\n",
    "#         # Start the thread to read frames from the video stream\n",
    "#         Thread(target=self.update, args=()).start()\n",
    "#         return self\n",
    "    \n",
    "#     def update(self):\n",
    "#         while True:\n",
    "#             if self.stopped:\n",
    "#                 return\n",
    "            \n",
    "#             (self.grabbed, self.frame) = self.stream.read()\n",
    "            \n",
    "#     def read(self):\n",
    "#         return self.frame\n",
    "    \n",
    "#     def stop(self):\n",
    "#         self.stopped = True  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
